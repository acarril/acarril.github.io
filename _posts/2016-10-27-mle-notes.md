---
layout: post
title:  "Some notes on maximum likelihood estimation"
categories: metrics R
---

[Maximum likelihood estimators](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLEs) are widely used in econometric analysis. Although their mathematical derivation is found in many textbooks, I've always wanted to explore the process from a data-driven perspective. In this post I'll go over the intuition of what MLEs do, using R (and a bit of math) to better understand the underpinnings of this method.

# Basic intuition on maximum likelihood

You have some data and are willing (?) to assume that it is generated from some distribution. Say you assume it comes from a normal (Gaussian) distribution. That's fine, but there are infinitely many possible parameters (i.e. means, variances) that "source" distribution may have:

![Possible normal distributions](/files/convergence_notes-gaussian_dists.png "Which distribution is producing my data?")

The idea behind MLE is to pick the distribution that is "most consistent" with the data. That is, *ML estimation finds the most likely function that explains the observed data.*

### How can you choose parameters which are "most consistent" with the data?

Say you've got a variable `y=4,1,10`. The most consistent parameters would be to choose a mean equal to 5 and a variance of 14. Yes, some other Gaussian distribution may have produced those values in `y`, but the key is that the probability of getting those particular values of is maximized with the chosen mean and variance.

In a regression framework the mean is simply a linear function of the data. Consider the same vector `y` as before and a new variable `x=1,-1,3`. The mean is the fitted regression model $$x' \hat\beta$$, with $$\hat\beta=[2.75, 2.25]$$. You can easily check this in R:

```R
# Input data
y <- c(4, 1, 10)
x <- c(1, -1, 3)
exampledata <- data.frame(y,x)

# Fit the model
lm(y ~ x, data = exampledata)
## Coefficients:
## (Intercept)            x  
##        2.75         2.25
```

# The likelihood function maximization

Let's now expand our example a bit. The basic idea still is that we have some points in `x` and `y`, and we want to know the parameters $$\beta$$ and $$\sigma^2$$ that most likely fit the data. We're going to start by cheating and generating the data with known parameters:

```R
# The true parameters
beta <- 2
sigma2 <- 1

# Generate dataset
set.seed(123)
data   <- data.frame(x = runif(100, 1, 10))
data$y <- beta*data$x + rnorm(100, 0, sigma2)
plot(data$x, data$y, xlab="x", ylab="y")
```

### The likelihood function

Suppose we *don't* know the true parameters $$\beta$$ and $$\sigma^2$$ that generated our data (which is usually the case). In a MLE linear model, we assume the data points come from a normal (Gaussian) distribution with mean $$x \beta$$ and variance $$\sigma^2$$, that is,

$$ y \sim \mathcal{N}(x\beta, \sigma^2) $$

 We know the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function) of that normal distribution is

$$f(x; x\beta, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}\exp{\left(-\frac{(y_i-x_i\beta)^2}{2\sigma^2}\right)}$$

Now, the idea is to find the values for $$\beta$$ and $$\sigma^2$$ that maximize that probability of observing points $$(x_i,y_i)$$ in our data. That function is the likelihood function (usually denoted by $$\mathcal{L}$$) and it's simply the product over $$i$$ of the previous expression:

$$ \mathcal{L} = \prod_{i=1}^n y_i $$

That form of $$\mathcal{L}$$ is a bit unwieldy, so we usually prefer taking logs and maximize the so called log likelihood function:

$$\log(\mathcal{L}) = \sum_{i = 1}^n-\frac{n}{2}\log(2\pi) -\frac{n}{2}\log(\sigma^2) -
      \frac{1}{2\sigma^2}(y_i - x_i\beta)^2$$

That's much better! We can now take this function and write it down in R:

```R
# Log-likelihood function
linear.lik <- function(theta, y, X){
  n      <- nrow(X)
  k      <- ncol(X)
  beta   <- theta[1:k]
  sigma2 <- theta[k+1]
  e      <- y - X%*%beta
  logl   <- -.5*n*log(2*pi) - .5*n*log(sigma2) - ((t(e) %*% e) / (2*sigma2))
  return(-logl)
}
```

### Log-likelihood maximization

Before maximizing $$\log(\mathcal{L})$$, it is cool to see how different values of $$\beta$$ and $$\sigma^2$$ shape its surface. We'll plot it for values of $$\beta$$ and $$\sigma^2$$ going from 0 to 3, in increments of 0.1:

```R
# Plot of log-likelihood function for different values of beta and sigma2
surface <- list()
k <- 0
for(beta in seq(0, 3, 0.1)){
  for(sigma2 in seq(0, 3, 0.1)){
    k <- k + 1
    logL <- linear.lik(theta = c(0, beta, sigma2), y = data$y, X = cbind(1, data$x))
    surface[[k]] <- data.frame(beta = beta, sigma2 = sigma2, logL = -logL)
  }
}
surface <- do.call(rbind, surface)
library(lattice)
wireframe(logL ~ beta*sigma2,
          surface,
          colorkey=FALSE,
          drape=TRUE,
          scales = list(arrows = FALSE, z = list(arrows=TRUE)),
          col.regions = rainbow(100, s=1, v=1, start=0, end = max(1,100 - 1)/100, alpha=1))
```

A pretty neat plot comes out, as you can see below. Where's the maximum? From the plot it is not clear, but you'll agree there *is* a maximum point somewhere. If you squint, you'll probably notice it is somewhere in the ridge along $$\beta=2$$, with the value of $$\sigma^2$$ being less clear... (remember their *true* values?)

![Plot of likelihood function](/files/mle_max_plot.png "There is a maximum somewhere")

So this looks very promising!  We now ask R to find the parameters that actually maximize our log-likelihood function:

```R
# Find maximization points
linear.MLE <- nlm(f=linear.lik, p=c(1,1,1), hessian=TRUE, y=data$y, X=cbind(1, data$x))
linear.MLE$estimate
## [1] 0.001049617 1.990013519 0.920734469
```

Remember that the true parameters of the distribution are $$\beta=2$$ and $$\sigma^2=1$$. Notice that the maximization process yields a pretty accurate estimate for $$\beta$$ and $$\sigma^2$$. The first value comes very close to zero, which is not surprising: it corresponds to the intercept, which we have implicitly assumed to be equal to zero.

# Final remarks

### How different is MLE to OLS?

Under many circumstances, not much. In fact, *the OLS estimator is identical to the ML estimator under the normality assumption for the error terms*. To see this, recall we've been considering the model

$$ y = x \beta + \epsilon $$

Assuming $$\epsilon \sim N(0,\sigma^2)$$, the $$\log(\mathcal{L})$$ of $$y\mid x$$ can be reduced to

$$ \frac{-n}{2} \log(\sigma^{2}) - \frac{1}{2 \sigma^{2}} \sum_{i=1}^{n} (y_{i}-x_{i} \beta)^{2} $$

If the above expression is viewed as a function of only $$\beta$$, the maximizer coincides with that which minimizes

$$ \sum_{i=1}^{n} (y_{i}-x_{i} \beta)^{2} $$

If you want to check this, you can run `summary(lm(y ~ x, data))`. Seeing is believing!

### If they're the same, why bother?

We have seen what maximum likelihood estimators try to do and what assumptions they make. Under a Gaussian model MLE is equivalent to OLS, but the latter is only a particular case of the former. ML estimators can accommodate different distributions, they become minimum variance unbiased estimators as the sample size increases and they have approximate normal distributions and approximate sample variances that can be used to generate confidence bounds and hypothesis tests for the parameters.

In practice, a good grasp on ML methods may help you get a better sense of what you're asking R or Stata to do when trying to fit [quantile regressions](https://en.wikipedia.org/wiki/Quantile_regression), [negative binomial regressions](https://en.wikipedia.org/wiki/Negative_binomial_distribution) or [Heckman selection models](https://en.wikipedia.org/wiki/Heckman_correction). This will surely come in handy ~~when~~ if you're pulling your hair out while trying to figure out why your estimations won't converge!

# References
Hayashi, Fumio (2000).  *Econometrics*, Princeton University Press.
